1: The making_dataset.ipynb file is used exclusively to generate the dataset through the ChatGPT API. Its sole purpose is to make repeated API calls, process the responses, and save the generated outputs into CSV files. This notebook does not perform any downstream processing or aggregation; it is strictly responsible for dataset creation. Also, in the SYSTEM_MSG variable defined in the second cell of the code, the prompt rules mentioned there are not the most accurate or up-to-date version. I do not currently have a local copy of the finalized prompt rules. However, the correct and more detailed version of this file is available in the Uplifty Google Drive, where you can find the full set of refined prompt rules used for more accurate label extraction.

2: The datasetcombining.ipynb file is designed to combine the CSV files produced by the making_dataset.ipynb notebook into a single consolidated dataset. Because data extraction through the API can occasionally halt due to rate limits or usage constraints, the first notebook may need to be re-run multiple times. Each re-run can result in new CSV files being generated, and this second notebook ensures that these new files are appended and merged without overwriting any previously created CSV files.

3: clean_hm.csv is a CSV derived from the HappyDB dataset (Kaggle), where each row contains a short 1â€“2 sentence entry. It includes 100,000+ records in total. To make processing manageable, I split this file into 10 separate CSV files with ~10,000 records each, which is why the two .ipynb files reference filenames like split_file_8.csv.